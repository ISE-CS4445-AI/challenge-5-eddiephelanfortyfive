{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tw01le53fJ"
      },
      "source": [
        "# Challenge 5 - RNNs\n",
        "\n",
        "Welcome to challenge #5!\n",
        "\n",
        "In this challenge, you will implement a simple RNN classifier using an LSTM cell for sentiment analysis on the YelpReviewPolarity dataset (a binary sentiment classification dataset).\n",
        "\n",
        "Your model should include:\n",
        "- An **Embedding** layer (with a specified `padding_idx`)\n",
        "- An **LSTM** layer (with `batch_first=True`)\n",
        "- A **Fully Connected (fc)** layer to map the final hidden state to the output\n",
        "- A **Sigmoid** activation to produce a probability between 0 and 1\n",
        "\n",
        "Your tasks are:\n",
        "\n",
        "1. **Data Preprocessing & DataLoader Setup** (2 points):  \n",
        "   Import the YelpReviewPolarity dataset, tokenize the text, build a vocabulary, and create a DataLoader to supply batches to your model.\n",
        "\n",
        "2. **RNNClassifier** (2 points):  \n",
        "   Implement a class that builds and returns the RNN classifier model using the parameters provided.\n",
        "\n",
        "3. **Training and evaluating the model** (2 points):  \n",
        "   Implement a function that takes your model and trains it over a number of epochs and then tests it with sample yelp reviews.\n",
        "\n",
        "4. **Q&A Section** (3 points total, 1 point each):  \n",
        "   Answer three questions (in markdown) about your implementation and key concepts.\n",
        "\n",
        "When you are finished, the provided pytest tests at the end of this notebook will automatically evaluate your code.\n",
        "\n",
        "\n",
        "## Important Note on Environment:\n",
        "This challenge will <span style=\"color: red\">not work properly on Codespaces</span> because of the lack of GPU and pytorch support (at least I haven't figured out the setup yet). So for this challenge, you will open your repository's notebook in Google Collab and continue. You can do so by clicking the \"Open in Collab\" badge below and opening this notebook from your repository there.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "After completing the code part, please make sure to run the TESTING section of this notebook for the test cases. Again, the pytest on Github will fail. I shall manually enter your grades without the help of the autograder for this challenge referring to the test section at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEaoqZv1iv3B"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RPUgTEksiv3B",
        "outputId": "50e0193e-af9a-4a71-ff87-351290cf479f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 14:15:34--  https://drive.usercontent.google.com/download?id=0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg&export=download&authuser=0&confirm=t&uuid=08839d6e-0170-44f8-a1c1-2f829c484617&at=AIrpjvOJpeXNKY4yGqP9mw6bXpQS:1739966900676\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.200.132, 2404:6800:4003:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 166373322 (159M) [application/octet-stream]\n",
            "Saving to: ‘yelp_review_polarity_csv.tar’\n",
            "\n",
            "yelp_review_polarit 100%[===================>] 158.67M   148MB/s    in 1.1s    \n",
            "\n",
            "2025-02-19 14:15:37 (148 MB/s) - ‘yelp_review_polarity_csv.tar’ saved [166373322/166373322]\n",
            "\n",
            "yelp_review_polarity_csv/\n",
            "yelp_review_polarity_csv/readme.txt\n",
            "yelp_review_polarity_csv/test.csv\n",
            "yelp_review_polarity_csv/train.csv\n",
            "Collecting torchdata==0.6.1\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting portalocker==2.7.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchdata==0.6.1)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchdata==0.6.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchdata==0.6.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchdata==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchdata==0.6.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchdata==0.6.1) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchdata==0.6.1)\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/e6/9d/dd0cdcd800e642e3c82ee3b5987c751afd4f3fb9cc2752517f42c3bc6e49/nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchdata==0.6.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchdata==0.6.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata==0.6.1) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->torchdata==0.6.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "  Downloading torchtext-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading torchtext-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->torchdata==0.6.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->torchdata==0.6.1) (1.3.0)\n",
            "Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, portalocker, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 portalocker-2.7.0 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "# Downloading the yelp review dataset\n",
        "!wget \"https://drive.usercontent.google.com/download?id=0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg&export=download&authuser=0&confirm=t&uuid=08839d6e-0170-44f8-a1c1-2f829c484617&at=AIrpjvOJpeXNKY4yGqP9mw6bXpQS:1739966900676\" -O yelp_review_polarity_csv.tar\n",
        "\n",
        "# Extracting the dataset\n",
        "!tar -xvf yelp_review_polarity_csv.tar\n",
        "\n",
        "# Downloading python package dependencies\n",
        "!pip install torchdata==0.6.1 torchtext portalocker==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "77zIg-Xhiv3B",
        "outputId": "64674d1d-bab1-4a7b-84d9-09475a676916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f48babd1ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import YelpReviewPolarity\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import csv\n",
        "\n",
        "# Set random seed for reproducibility.\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospkYBj2iv3B"
      },
      "source": [
        "## Task 1: Data Preprocessing (2 points)\n",
        "\n",
        "Note: the YelpReviewPolarity dataset label includes\n",
        "- 1 : Negative polarity.\n",
        "- 2 : Positive polarity.\n",
        "\n",
        "Refer the [pytorch docs](https://pytorch.org/text/0.8.1/datasets.html#yelpreviewpolarity) for the dataset for more info.\n",
        "\n",
        "The below code cell downloads the dataset and loads it for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JF3iwGOZiv3B",
        "outputId": "5fd45ae2-a2a6-480b-fdf2-5f973189136e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 50000\n",
            "Number of testing examples: 38000\n"
          ]
        }
      ],
      "source": [
        "def load_local_yelp_list(train_csv_path, test_csv_path, has_header=True, sample_size=50000):\n",
        "    \"\"\"\n",
        "    Reads the local train.csv and test.csv for Yelp Review Polarity\n",
        "    and returns two lists: train_list, test_list,\n",
        "    where each element is (label, text).\n",
        "    label is int (1 or 2), text is the review string.\n",
        "    \"\"\"\n",
        "    train_list = []\n",
        "    with open(train_csv_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        if has_header:\n",
        "            next(reader, None)  # skip the header row\n",
        "        for row in reader:\n",
        "            if len(train_list) >= sample_size:\n",
        "                break\n",
        "            label_str, text = row\n",
        "            label = int(label_str)\n",
        "            train_list.append((label, text))\n",
        "\n",
        "    test_list = []\n",
        "    with open(test_csv_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        if has_header:\n",
        "            next(reader, None)\n",
        "        for row in reader:\n",
        "            label_str, text = row\n",
        "            label = int(label_str)\n",
        "            test_list.append((label, text))\n",
        "\n",
        "    return train_list, test_list\n",
        "\n",
        "train_list, test_list = load_local_yelp_list('./yelp_review_polarity_csv/train.csv', './yelp_review_polarity_csv/test.csv', has_header=False)\n",
        "print(f\"Number of training examples: {len(train_list)}\")\n",
        "print(f\"Number of testing examples: {len(test_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GEDCs6cViv3C",
        "outputId": "e4141a07-348f-4978-a32d-2d00c69a5274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\")\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "print(train_list[0])\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(train_list),\n",
        "    specials=['<unk>', '<pad>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def text_pipeline(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "def label_pipeline(label):\n",
        "    return 1 if label == 2 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GEK4WO4Hiv3C"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Create a custom Dataset and DataLoader\n",
        "# -------------------------\n",
        "class YelpDataset(Dataset):\n",
        "    def __init__(self, data_iter):\n",
        "        # TODO: Store the data and process it if necessary.\n",
        "        self.data = [(text_pipeline(text), label_pipeline(label)) for label, text in data_iter]\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Return the total number of examples.\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Return the processed (text_tensor, label) tuple for index idx.\n",
        "        return self.data[idx]\n",
        "\n",
        "# TODO: Create a collate function that pads sequences in a batch.\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(torch.tensor(_text, dtype=torch.int64))\n",
        "\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# TODO: Create a DataLoader for the training data.\n",
        "batch_size = 32\n",
        "train_dataset = YelpDataset(train_list)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gDDx-hQiv3C"
      },
      "source": [
        "## Task 2: Build the RNN Classifier (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cOBI7x6Liv3C"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the RNNClassifier class with no implementation provided.\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, output_dim: int, num_layers: int, padding_idx: int):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        # TODO: Create an Embedding layer.\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "        # TODO: Create an LSTM layer (batch_first=True).\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        # TODO: Create a fully connected layer mapping hidden_dim to output_dim.\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        # TODO: Create a Sigmoid activation.\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, seq_length]\n",
        "        # TODO: Convert token IDs into embeddings.\n",
        "        embedded = self.embedding(text)\n",
        "        # TODO: Process the embeddings through the LSTM.\n",
        "        output, (hidden, cell) = self.lstm(embedded)  # Your code here.\n",
        "        # TODO: Extract the final hidden state from the last LSTM layer.\n",
        "        hidden_last = hidden[-1]  # Your code here.\n",
        "        # TODO: Map the hidden state to output using the fully connected layer.\n",
        "        logits = self.fc(hidden_last)  # Your code here.\n",
        "        # TODO: Apply the Sigmoid activation and return the squeezed output.\n",
        "        return self.sigmoid(logits).squeeze()  # Your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOd8Ur8tiv3C"
      },
      "source": [
        "## Task 3: Training and evaluating your model (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GtVICsnniv3C"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate():\n",
        "    # TODO: Define the device (e.g., \"cuda\")\n",
        "    device = None\n",
        "\n",
        "    # Use the vocabulary built in the Data Preprocessing section.\n",
        "    vocab_size = len(vocab)\n",
        "    embed_dim = 100        # You can choose a value (e.g., 100)\n",
        "    hidden_dim = 128       # You can choose a value (e.g., 128)\n",
        "    output_dim = 1         # For binary classification, output dimension is 1\n",
        "    num_layers = 1         # Number of LSTM layers, e.g., 1\n",
        "    padding_idx = vocab['<pad>']\n",
        "\n",
        "    # TODO: Build your RNN classifier using your RNNClassifier class.\n",
        "    model = RNNClassifier(vocab_size, embed_dim, hidden_dim, output_dim, num_layers, padding_idx)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # TODO: Define your loss function\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # TODO: Define your optimizer with a chosen learning rate.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Set the number of epochs for training.\n",
        "    num_epochs = 10  # You can adjust the number of epochs.\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TODO: Set the model to training mode.\n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0.0  # Initialize epoch loss.\n",
        "\n",
        "        # Iterate over batches in your training DataLoader.\n",
        "        for batch_idx, (text_batch, label_batch) in enumerate(train_loader):\n",
        "            # TODO: Move text_batch and label_batch to the defined device.\n",
        "            text_batch, label_batch = text.batch.to(device), label.batch.to(device)\n",
        "\n",
        "            # TODO: Zero out the gradients.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # TODO: Perform a forward pass: obtain predictions from the model.\n",
        "            predictions = model(text_batch)\n",
        "\n",
        "            # TODO: Compute the loss using your criterion.\n",
        "            loss = criterion(predictions, label_batch.float())\n",
        "\n",
        "            # TODO: Perform the backward pass to compute gradients.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # TODO: Accumulate the loss for reporting.\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # TODO: Update the model parameters.\n",
        "\n",
        "\n",
        "            # TODO: Accumulate the loss for reporting.\n",
        "\n",
        "        # Compute the average loss for the epoch and print it.\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Evaluation on Sample Yelp Reviews\n",
        "    # -------------------------\n",
        "\n",
        "    # Define some sample Yelp reviews for evaluation.\n",
        "    sample_reviews = [\n",
        "        \"The food was amazing and the service was excellent!\",\n",
        "        \"I did not enjoy my visit at all. The experience was terrible.\",\n",
        "        \"The ambiance was pleasant but the food was just okay.\",\n",
        "        \"Absolutely loved the place! Will come back again.\"\n",
        "    ]\n",
        "\n",
        "    # TODO: Set the model to evaluation mode.\n",
        "\n",
        "    predictions = []  # Store the predictions for the sample reviews.\n",
        "    print(\"Evaluation on Sample Yelp Reviews:\")\n",
        "    for review in sample_reviews:\n",
        "        with torch.no_grad():\n",
        "            # TODO: Convert the review text to a tensor using your text_pipeline function.\n",
        "            review_tensor = torch.tensor(text_pipeline(review), dtype=torch.int64)\n",
        "            # TODO: Add a batch dimension to review_tensor (e.g., using unsqueeze).\n",
        "            review_tensor =  review_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "            # TODO: Perform a forward pass to get the prediction.\n",
        "            prediction = model(review_tensor)\n",
        "\n",
        "            # TODO: Interpret the prediction\n",
        "            sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
        "\n",
        "            # Print the review and its predicted sentiment along with the prediction score.\n",
        "            print(f\"Review: {review}\\nPredicted Sentiment: {sentiment} (Score: {prediction.item():.4f})\\n\")\n",
        "\n",
        "            # TODO: Append the prediction score (as a float) to the predictions list.\n",
        "            predictions.append(prediction.item())\n",
        "\n",
        "    # Return a dictionary with keys \"avg_loss\" and \"predictions\".\n",
        "    return {\"avg_loss\": avg_loss, \"predictions\": predictions}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXnhlk-Qiv3C"
      },
      "source": [
        "## Task 4: Q&A Section (3 points)\n",
        "\n",
        "Please answer the questions below in brief. Each carries 1 point. This section is also open-book i.e., you can refer documentation to inform your response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx-ggSf0iv3C"
      },
      "source": [
        "**Question 1:**  \n",
        "What is the purpose of specifying a `padding_idx` in the Embedding layer, and how does it affect the model's training and output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9270d8vciv3C"
      },
      "source": [
        "<font color='red'>*Your Answer:* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfTyzS__iv3C"
      },
      "source": [
        "> *Ensures that the embedding for the padded token doesnt change or update during training. This prevents the padding from effecting the learning of the model.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4lpzwaliv3C"
      },
      "source": [
        "**Question 2:**  \n",
        "Why do we use the final hidden state from the LSTM for classification? How does this hidden state encapsulate the overall information of the input sequence in the context of sentiment analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UanM-h17iv3C"
      },
      "source": [
        "<font color='red'>*Your Answer:* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXwM_qhYiv3C"
      },
      "source": [
        "> *This state represents the overall sentiment by aggregating relevant features from all previous time steps.\n",
        "\n",
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTX9qeKNiv3C"
      },
      "source": [
        "**Question 3:**  \n",
        "What is the role of the Sigmoid activation in this binary classification model? How might this change if you were working on a multi-class classification task? (think of other activation functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNfNBkJQiv3C"
      },
      "source": [
        "<font color='red'>*Your Answer:* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2y5vfXWiv3C"
      },
      "source": [
        "> *The Sigmoid activation maps the output to a probability between 0 and 1 for binary classification. For multi-class classification, Softmax would replace Sigmoid, as it converts outputs into a probability distribution over multiple classes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl1OZX_Tiv3C"
      },
      "source": [
        "---\n",
        "# Autograder section.\n",
        "\n",
        "After you finish your code implementation, please run this part of the notebook to see your score for the coding section (6 points)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}